{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5jm9d4Kr4rg"
      },
      "source": [
        "https://python.langchain.com/v0.1/docs/get_started/introduction/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8APBeXuh59XL"
      },
      "source": [
        "!pip install python-dotenv\n",
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYVddq1kaFM2"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GYWcLAOpAS1"
      },
      "outputs": [],
      "source": [
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFTqbWsdXAGk"
      },
      "outputs": [],
      "source": [
        "# Se cargan los LLMs\n",
        "\n",
        "from langchain_openai  import ChatOpenAI # Para chat models\n",
        "from langchain_openai import OpenAI # Para completion models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0V8Qb11XAGl"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(\"Cual es la capital de Argentina\") # En este caso se usa un modelo conversacional\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jzweuEiXAGm"
      },
      "source": [
        "# 1- PROMPT + TEMPLATES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5M4T4SNJXAGn"
      },
      "source": [
        "## A- PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okjkASekXAGo"
      },
      "source": [
        "* `PromptTemplate` consta de un `template` en formato de `str`.\n",
        "* Acepta un conjunto de parámetros del usuario que se pueden utilizar para generar un mensaje para un `LLM`.\n",
        "* El `template` se puede formatear usando `f-strings`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfEVoKAWXAGo"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcEMkjUSXAGp"
      },
      "source": [
        "**Usando un modelo conversacional:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meHT35ddXAGp"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Tell me a joke about {topic}, make it funny\n",
        "and in {language}\n",
        "\"\"\"\n",
        "print(f\"template:{template}\")\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template = template) # En este ejemplo, la variable 'template' es un 'str'.\n",
        "print(f\"prompt_template:\\n{prompt_template}\")\n",
        "\n",
        "print()\n",
        "\n",
        "prompt = prompt_template.format(topic = \"programation\", language = \"german\")\n",
        "print(f\"prompt:{prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EpqSjqdXAGq"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAMaHQiXXAGq"
      },
      "outputs": [],
      "source": [
        "# Se repite el ejemplo, pero sin variables\n",
        "\n",
        "template = \"\"\"\n",
        "Tell me a joke\n",
        "\"\"\"\n",
        "print(f\"template:{template}\")\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template = template)\n",
        "print(f\"prompt_template:\\n{prompt_template}\")\n",
        "\n",
        "print()\n",
        "\n",
        "prompt = prompt_template.format() # En este caso no pasamos ninguna variable\n",
        "print(f\"prompt:{prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUKTIZuVXAGq"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S811Y0yaXAGq"
      },
      "source": [
        "**Usando un modelo de completion:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCJeZYUbXAGq"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "Escribí una función llamada '{nombre_funcion}' en código {language}\n",
        "que ejecute la siguiente:\n",
        "###\n",
        "{tarea}\n",
        "###\n",
        "\"\"\"\n",
        "print(f\"template:{template}\")\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template = template)\n",
        "print(f\"prompt_template:\\n{prompt_template}\")\n",
        "\n",
        "print()\n",
        "\n",
        "prompt = prompt_template.format(nombre_funcion = \"imprimir_numeros\",\n",
        "                                language = \"python\",\n",
        "                                tarea = \"Imprimí los números del 1 al 10\")\n",
        "print(f\"prompt:{prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9qwcPQCXAGr"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\", temperature = 0)\n",
        "\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCuQAfQ7XAGr"
      },
      "source": [
        "## B- ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOEJFYPiXAGr"
      },
      "source": [
        "* `ChatPromptTemplate` se compone de una lista de mensajes.\n",
        "* Cada elemento nuevo es un mensaje nuevo en el mensaje final.\n",
        "* Cada mensaje tiene tiene dos parámetros: `role` y `content`.\n",
        "* `Message`: cuando no hay variables para formatear.\n",
        "* `MessageTemplate`: cuando hay variables para formatear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-4Md0uxXAGr"
      },
      "source": [
        "**Usando únicamente ChatPromptTemplate:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSMThbTFXAGr"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yW2OXUoXAGr"
      },
      "outputs": [],
      "source": [
        "# Notar que estoy definiendo de forma manual si el mensaje es de tipo:\n",
        "    # 'system'\n",
        "    # 'human'\n",
        "    # 'ai'\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"), # Hay variables para formatear\n",
        "        (\"human\", \"Hello, how are you doing?\"),\n",
        "        (\"ai\", \"I'm doing well, thanks!\"),\n",
        "        (\"human\", \"{user_input}\"),\n",
        "    ]\n",
        ")\n",
        "print(f\"chat_template:\")\n",
        "for i in chat_template:\n",
        "    print(i)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMB8egieMy6e"
      },
      "outputs": [],
      "source": [
        "chat_template.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eENYAYjEXAGr"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(name = \"Bob\", user_input = \"What is your name?\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYlxg9TeXAGr"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU69t-C_XAGs"
      },
      "source": [
        "**Usando únicamente 'Message' (no hay variables para formatear):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSZCdFiIXAGs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XnKaq10XAGs"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        SystemMessage(content = \"You are a helpful AI bot. Your name is Bob.\"), # Pasamos la variable de forma manual\n",
        "        HumanMessage(content = \"Hello, how are you doing?\"),\n",
        "        AIMessage(content = \"I'm doing well, thanks!\"),\n",
        "        HumanMessage(content = \"What is your name?\"), # Pasamos la variable de forma manual\n",
        "    ]\n",
        ")\n",
        "print(f\"chat_template:\")\n",
        "for i in chat_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c6bQz5VXAGs"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages()\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bD9X52xXAGs"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfeL04OEXAGs"
      },
      "source": [
        "**Usando únicamente 'MessagePromptTemplate' (hay variables para formatear):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uba4iVhqXAGs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iVLWOVf8XAGs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import HumanMessagePromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template = \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "        HumanMessagePromptTemplate.from_template(template = \"{user_input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"chat_template:\")\n",
        "for i in chat_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KKsxsWUXAGs"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(name = \"Bob\", user_input = \"What is your name?\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt-MNlhTXAGs"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIpYAwGlXAGt"
      },
      "source": [
        "**Combinando 'Message' con 'MessagePromptTemplate':**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apSyUTOcXAGt"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_core.prompts import HumanMessagePromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-bQYnLPXAGt"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template = \"You are a helpful AI bot. Your name is {name}.\"), # Como hay variables para formatear se utiliza 'MessagePromptTemplate'\n",
        "        HumanMessage(content = \"Hello, how are you doing?\"), # Como no hay variables para formatear se utiliza 'Message'\n",
        "        AIMessage(content = \"I'm doing well, thanks!\"), # Como no hay variables para formatear se utiliza 'Message'\n",
        "        HumanMessagePromptTemplate.from_template(template = \"{user_input}\"), # Como hay variables para formatear se utiliza 'MessagePromptTemplate'\n",
        "    ]\n",
        ")\n",
        "print(f\"chat_template:\")\n",
        "for i in chat_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT7_VF4HXAGt"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(name = \"Bob\", user_input = \"What is your name?\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvPPL-L9XAGt"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jtv7YZdeXAGt"
      },
      "source": [
        "**Otro ejemplo combinando 'Message' con 'MessagePromptTemplate':**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4522mjIXAGx"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        SystemMessage(content = \"Responde con formato json.\"), # Como no hay variables para formatear se utiliza 'Message'\n",
        "        HumanMessagePromptTemplate.from_template(template = \"Nombra los {n} países de {area} con mayor cantidad de habitantes.\") # Como hay variables para formatear se utiliza 'MessagePromptTemplate'\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yslyw5gXAGx"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(n = \"10\", area = \"Europe\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vzobz0iyXAGx"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVKl0PtKQOjS"
      },
      "outputs": [],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEUI9UTdXAGx"
      },
      "source": [
        "**¿Cómo podemos definir el formato 'json' como una variable a pasar?:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxWx0P19XAGx"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(messages =\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(template = \"Responde con formato {formato}.\"),\n",
        "        HumanMessagePromptTemplate.from_template(template = \"Nombra los {n} países de {area} con mayor cantidad de habitantes.\")\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPgNM3h7XAGy"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(formato = \"json\", n = \"10\", area = \"Europe\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HApZotdQXAGy"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI()\n",
        "output = llm.invoke(messages)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhlAUFnjXAGy"
      },
      "source": [
        "# 2- TÉCNICAS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPNz7fsyXAGy"
      },
      "source": [
        "## A- Zero-Shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q24PWUrJXAGy"
      },
      "outputs": [],
      "source": [
        "# Recordar la estructura template --> prompt_template --> prompt\n",
        "\n",
        "template = \"\"\"\n",
        "The following is a conversation with an AI assistant.\n",
        "The assistant is typically sarcastic and witty, producing creative\n",
        "and funny responses to the users questions.\n",
        "\n",
        "User: {query}\n",
        "AI:\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    query = \"What is the meaning of life?\"\n",
        ")\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb-h3LsmXAGy"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4nK5t_UXAGy"
      },
      "source": [
        "## B- Few-Shot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXGNnw-TXAGy"
      },
      "source": [
        "* `Langchain` provee una clase para poder utilizar este tipo de técnicas: `FewShotPromptTemplate`.\n",
        "* El objetivo de `FewShotPromptTemplate` es seleccionar ejemplos basados en el input y luego formatear los ejemplos en un prompt final, el cual alimentará el `LLM`.\n",
        "* Como se observó en ejemplos anteriores se puede usar con `PromptTemplate` o con `MessagePromptTemplate`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S1-4bf9XAGy"
      },
      "source": [
        "**EJEMPLO 1 con PromptTemplate:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuKsDrCOXAGy"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.few_shot import FewShotPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_BkoMW-XAGy"
      },
      "outputs": [],
      "source": [
        "# Creamos los ejemplos:\n",
        "examples = [\n",
        "    {\n",
        "        \"query\": \"How are you?\",\n",
        "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
        "    }, {\n",
        "        \"query\": \"What time is it?\",\n",
        "        \"answer\": \"It's time to get a watch.\"\n",
        "    }, {\n",
        "        \"query\": \"What is the meaning of life?\",\n",
        "        \"answer\": \"42\"\n",
        "    }, {\n",
        "        \"query\": \"What is the weather like today?\",\n",
        "        \"answer\": \"Cloudy with a chance of memes.\"\n",
        "    }, {\n",
        "        \"query\": \"What is your favorite movie?\",\n",
        "        \"answer\": \"Terminator\"\n",
        "    }, {\n",
        "        \"query\": \"Who is your best friend?\",\n",
        "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
        "    }, {\n",
        "        \"query\": \"What should I do today?\",\n",
        "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZBwbBBiXAGz"
      },
      "outputs": [],
      "source": [
        "# Creamos el template para los ejemplos\n",
        "example_template = \"\"\"\n",
        "User: {query}\n",
        "AI: {answer}\n",
        "\"\"\"\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "template =\n",
        "Escribí una función llamada 'imprimir_numeros' en código {language}\n",
        "que ejecute la siguiente {tarea}.\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWjyb_UvTew-"
      },
      "outputs": [],
      "source": [
        "print(example_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xF7GwNnXAGz"
      },
      "outputs": [],
      "source": [
        "# Creamos el prompt_template para los ejemplos\n",
        "example_prompt = PromptTemplate(input_variables = [\"query\", \"answer\"], template = example_template)\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate.from_template(template = template)\n",
        "\"\"\"\n",
        "print(example_prompt.format(**examples[0]))\n",
        "print(\"---\" * 20)\n",
        "print(example_prompt.format(**examples[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHo5pAoAXAGz"
      },
      "outputs": [],
      "source": [
        "# Agregamos el prompt_template de los ejemplos al prompt_template final, previo a esto agregamos 'prefijo' y 'sufijo':\n",
        "\n",
        "# Prefijo es como si le dijeramos cómo tiene que ser el comportamiento\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "# Sufijo es para pasar el formato de input y ouput\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# Creamos el prompt_template final\n",
        "prompt_template = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = prefix,\n",
        "    suffix = suffix,\n",
        "    input_variables = [\"query\"],\n",
        "    example_separator = \"\\n\\n\"\n",
        ")\n",
        "\n",
        "for i in prompt_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4qt-zikXAGz"
      },
      "outputs": [],
      "source": [
        "prompt  = prompt_template.format(query = \"What is the meaning of life?\")\n",
        "print(prompt)\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "prompt = prompt_template.format(language = \"python\",\n",
        "                                tarea = \"Imprimí los números del 1 al 10\")\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIIo0E2YXAGz"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB5w5-o3XAGz"
      },
      "source": [
        "**EJEMPLO 2 con PromptTemplate:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI59nHbUXAGz"
      },
      "outputs": [],
      "source": [
        "# Creamos los ejemplos:\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: How old was Muhammad Ali when he died?\n",
        "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "Follow up: How old was Alan Turing when he died?\n",
        "Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "So the final answer is: Muhammad Ali\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the founder of craigslist born?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who was the founder of craigslist?\n",
        "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "Follow up: When was Craig Newmark born?\n",
        "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "So the final answer is: December 6, 1952\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who was the mother of George Washington?\n",
        "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "Follow up: Who was the father of Mary Ball Washington?\n",
        "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "So the final answer is: Joseph Ball\n",
        "\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "Are follow up questions needed here: Yes.\n",
        "Follow up: Who is the director of Jaws?\n",
        "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "Follow up: Where is Steven Spielberg from?\n",
        "Intermediate Answer: The United States.\n",
        "Follow up: Who is the director of Casino Royale?\n",
        "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "Follow up: Where is Martin Campbell from?\n",
        "Intermediate Answer: New Zealand.\n",
        "So the final answer is: No\n",
        "\"\"\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af0DXtBIXAGz"
      },
      "outputs": [],
      "source": [
        "# Creamos el template para los ejemplos\n",
        "example_template = \"\"\"\n",
        "Question: {question}\n",
        "{answer}\n",
        "\"\"\"\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "template =\n",
        "Escribí una función llamada 'imprimir_numeros' en código {language}\n",
        "que ejecute la siguiente {tarea}.\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4o5gCFz-XAG0"
      },
      "outputs": [],
      "source": [
        "# Creamos el prompt_template para los ejemplos\n",
        "example_prompt = PromptTemplate(input_variables = [\"question\", \"answer\"], template = example_template)\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate.from_template(template = template)\n",
        "\"\"\"\n",
        "print(example_prompt.format(**examples[0]))\n",
        "print(\"---\" * 20)\n",
        "print(example_prompt.format(**examples[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRXT4TjMXAG0"
      },
      "outputs": [],
      "source": [
        "# Agregamos el prompt_template de los ejemplos al prompt_template final, previo a esto agregamos 'prefijo' y 'sufijo':\n",
        "\n",
        "# Prefijo es como si le dijeramos cómo tiene que ser el comportamiento\n",
        "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
        "assistant. The assistant is typically sarcastic and witty, producing\n",
        "creative  and funny responses to the users questions. Here are some\n",
        "examples:\n",
        "\"\"\"\n",
        "\n",
        "# Sufijo es para pasar el formato de input y ouput\n",
        "suffix = \"\"\"\n",
        "User: {query}\n",
        "AI: \"\"\"\n",
        "\n",
        "# Creamos el prompt_template final\n",
        "prompt_template = FewShotPromptTemplate(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    suffix = \"Question: {input}\",\n",
        "    input_variables = [\"input\"]\n",
        ")\n",
        "\n",
        "for i in prompt_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USWifwkEXAG0"
      },
      "outputs": [],
      "source": [
        "prompt  = prompt_template.format(input = \"Who was the father of Mary Ball Washington?\")\n",
        "print(prompt)\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "prompt = prompt_template.format(language = \"python\",\n",
        "                                tarea = \"Imprimí los números del 1 al 10\")\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DLqpYB8XAG0"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "output = llm.invoke(prompt)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sleBpVQXAG0"
      },
      "outputs": [],
      "source": [
        "prompt  = prompt_template.format(input = \"Are both the players Lebron James and Cristiano Ronaldo from the same country?\")\n",
        "print(prompt)\n",
        "\n",
        "# Acá es cómo habíamos hecho en los primeros ejemplos\n",
        "\"\"\"\n",
        "prompt = prompt_template.format(language = \"python\",\n",
        "                                tarea = \"Imprimí los números del 1 al 10\")\n",
        "\"\"\"\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PsqXAGfsXAG0"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "output = llm.invoke(prompt)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL9-XUILXAG0"
      },
      "source": [
        "**EJEMPLO 3 con MessagePromptTemplate:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NJvCHhMXAG0"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHR7kUPcXAG0"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"2+2\", \"output\": \"4\"},\n",
        "    {\"input\": \"2+3\", \"output\": \"5\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl4Gt5iMXAG1"
      },
      "outputs": [],
      "source": [
        "# This is a prompt template used to format each individual example.\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSEXJ8awXAG1"
      },
      "outputs": [],
      "source": [
        "chat_template_examples = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt = example_prompt,\n",
        "    examples = examples,\n",
        ")\n",
        "\n",
        "print(chat_template_examples.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ9cMLzOXAG1"
      },
      "outputs": [],
      "source": [
        "chat_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
        "        chat_template_examples,\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "print(f\"chat_template:\")\n",
        "for i in chat_template:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwaVL8ROXAG1"
      },
      "outputs": [],
      "source": [
        "messages = chat_template.format_messages(input = \"What is the surface area of a circle whose radius is 2 cm?\")\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwTjb-MrXAG1"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "\n",
        "output = llm.invoke(messages)\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yrj_y3yhXAG1"
      },
      "source": [
        "## C- Prompt Role"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O5qg1uBXAG1"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You're Carlitos, a friendly and helpful chatbot. Your role is to respond to all comments made to you. Before respond to the user in a sarcastically way, please introduce yourself with a short presentation\n",
        "Here is the question: {query}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    query = \"Why did the chicken cross to the other side?\"\n",
        ")\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6_YrEyqXAG1"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVUwAB2sXAG1"
      },
      "source": [
        "## D- Chain of thought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LJeYibDXAG2"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\"\n",
        "Para la siguiente tarea:\n",
        "{tarea}\n",
        "Describe tu razonamiento paso a paso, en bullets.\n",
        "\"\"\"\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "\n",
        "query = \"\"\"\n",
        "Un salón de clases tiene dos sillas azules por cada tres sillas rojas.\n",
        "Si hay un total de 30 sillas en el salón de clases, ¿cuántas sillas azules hay?\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    tarea = query\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")#, temperature = 0)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7vhrC7TXAG2"
      },
      "source": [
        "## E- Least to most"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RAH2_h8XAG2"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\"\n",
        "Dada la siguiente consulta:\n",
        "{consulta}\n",
        "\n",
        "Responde a la siguiente instrucción:\n",
        "{instruccion}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\n",
        "    template = template\n",
        ")\n",
        "\n",
        "consulta = \"\"\"\n",
        "Compré una remera el 1 de marzo. Vi que tenía descuento, así que compré una camisa que originalmente costaba $30 y obtuve un 40% de descuento.\n",
        "Vi que tenés un nuevo descuento para remeras del 50%.\n",
        "Me pregunto si puedo devolver la camiseta y tener suficiente crédito en la tienda para comprar dos de tus remeras.\n",
        "\"\"\"\n",
        "\n",
        "instruccion = \"\"\"\n",
        "Sos un agente de servicio al cliente encargado de responder amablemente a las consultas de los clientes.\n",
        "Se permiten devoluciones dentro de los 30 días. La fecha de hoy es 29 de marzo. Actualmente hay un 50% de descuento en todas las remeras.\n",
        "Los precios oscilan entre $ 18 y $ 100.\n",
        "No inventes ninguna información sobre políticas de descuento. ¿Qué subproblemas deben resolverse antes de responder la consulta?\n",
        "Resuelve cada uno de ellos para dar la respuesta final.\n",
        "\"\"\"\n",
        "\n",
        "prompt = prompt_template.format(\n",
        "    consulta = consulta,\n",
        "    instruccion = instruccion\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXnW6WIyWkAu"
      },
      "outputs": [],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Mmp1d-Wj-R"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\")#, temperature = 0)\n",
        "output = llm.invoke(prompt)\n",
        "print(output.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNgMuBTVXAG2"
      },
      "source": [
        "# 3- ALGUNAS ESTRATEGIAS ADICIONALES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PgyHJQIXAG2"
      },
      "source": [
        "## A- Seleccionando por longitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZTcYaIXAG2"
      },
      "source": [
        "* Selecciona qué ejemplos usar según la longitud.\n",
        "* Esto es útil cuando nos preocupa crear un mensaje que abarque toda la ventana de contexto.\n",
        "* Para entradas más largas, seleccionará menos ejemplos para incluir, mientras que para entradas más cortas seleccionará más."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBAkYm2jXAG2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.example_selectors import LengthBasedExampleSelector\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sttlJYNeXAG2"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeiMwDGpXAG2"
      },
      "outputs": [],
      "source": [
        "example_template = \"\"\"\n",
        "Input: {input}\n",
        "Output: {output}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7eJp4AzXAG2"
      },
      "outputs": [],
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template = example_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKrm9CGHXAG2"
      },
      "outputs": [],
      "source": [
        "example_selector = LengthBasedExampleSelector(\n",
        "    examples = examples,\n",
        "    example_prompt = example_prompt,\n",
        "    max_length = 25 # get_text_length: Callable[[str], int] = lambda x: len(re.split(\"\\n| \", x))\n",
        ")\n",
        "# max_length es el limite maximo en palabras del la variable a formatear + ejemplos.\n",
        "    # Ejemplo 1: adjective = \"big\", 25 - 1: tengo 24 palabras para agregar como ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxIe5k4UXAG2"
      },
      "outputs": [],
      "source": [
        "prompt_template = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = \"Give the antonym of every input\",\n",
        "    suffix = \"Input: {adjective}\\nOutput:\",\n",
        "    input_variables = [\"adjective\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmFdJR4XAG3"
      },
      "outputs": [],
      "source": [
        "prompt = prompt_template.format(adjective = \"big\")\n",
        "print(len(re.split(\"\\n| \", prompt)))\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIYS5ZTfXAG3"
      },
      "outputs": [],
      "source": [
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mYK5_kJXAG3"
      },
      "source": [
        "## B- Seleccionando por similaridad del coseno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCGv-CP7XAG3"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
        "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
        "from langchain_openai import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKxTXmrUXAG3"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\"input\": \"happy\", \"output\": \"sad\"},\n",
        "    {\"input\": \"tall\", \"output\": \"short\"},\n",
        "    {\"input\": \"energetic\", \"output\": \"lethargic\"},\n",
        "    {\"input\": \"sunny\", \"output\": \"gloomy\"},\n",
        "    {\"input\": \"windy\", \"output\": \"calm\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqEBAtPWXAG3"
      },
      "outputs": [],
      "source": [
        "examples_template = \"\"\"\n",
        "Input: {input}\n",
        "Output: {output}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zab3BymWXAG3"
      },
      "outputs": [],
      "source": [
        "example_prompt = PromptTemplate(\n",
        "    input_variables = [\"input\", \"output\"],\n",
        "    template = examples_template,\n",
        ")\n",
        "for i in example_prompt:\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9tKb-_9XAG3"
      },
      "outputs": [],
      "source": [
        "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
        "    examples = examples,\n",
        "    embeddings = OpenAIEmbeddings(),\n",
        "    vectorstore_cls = Chroma,\n",
        "    k = 1,\n",
        ")\n",
        "example_selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIAmMlNBXAG3"
      },
      "outputs": [],
      "source": [
        "similar_prompt = FewShotPromptTemplate(\n",
        "    # We provide an ExampleSelector instead of examples.\n",
        "    example_selector = example_selector,\n",
        "    example_prompt = example_prompt,\n",
        "    prefix = \"Give the antonym of every input\",\n",
        "    suffix = \"Input: {adjective}\\nOutput:\",\n",
        "    input_variables = [\"adjective\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm5clCTVXAG4"
      },
      "outputs": [],
      "source": [
        "print(similar_prompt.format(adjective = \"worried\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXXmjH9tXAG4"
      },
      "outputs": [],
      "source": [
        "print(similar_prompt.format(adjective = \"large\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flyFgqnKXAG4"
      },
      "outputs": [],
      "source": [
        "prompt = similar_prompt.format(adjective = \"large\")\n",
        "llm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\")\n",
        "output = llm.invoke(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpOrFOooXAG4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0jzweuEiXAGm",
        "5M4T4SNJXAGn",
        "XhlAUFnjXAGy",
        "rPNz7fsyXAGy",
        "k4nK5t_UXAGy",
        "Yrj_y3yhXAG1",
        "tVUwAB2sXAG1",
        "K7vhrC7TXAG2",
        "wNgMuBTVXAG2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}